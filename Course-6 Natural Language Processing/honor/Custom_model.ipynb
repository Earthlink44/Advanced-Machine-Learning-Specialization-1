{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Custom model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0dqX1mnnVYOz",
        "sGoqoudfVYO3",
        "PbTUQW3XVYPD"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCmEZUnLVYNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import datasets\n",
        "import itertools\n",
        "import unicodedata\n",
        "import re\n",
        "import csv\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qITtE5VYNv",
        "colab_type": "text"
      },
      "source": [
        "In this work, I use PyTorch instead of TensorFlow. I feel more comfortable with PyTorch which is more easy to use. I was inspired by 2 tutorials :\n",
        "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "* https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spvl_Ql0VYNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sRMTzF-VYNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXynf25mVYN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOk4L-jKVYN5",
        "colab_type": "text"
      },
      "source": [
        "# Dataset loading\n",
        "\n",
        "We load `opensubs` dataset. Here, we use tools provided for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1IA_1uWVYN6",
        "colab_type": "code",
        "colab": {},
        "outputId": "aeca8e74-d116-4102-85e7-2297625ab73d"
      },
      "source": [
        "opensubs = datasets.readOpensubsData(path='data/opensubs', max_len=max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:   0%|          | 7/4637 [00:00<01:06, 69.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading OpenSubtitles conversations in data/opensubs.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  19%|█▉        | 886/4637 [00:27<03:12, 19.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Comedy/2003/529_124078_171007_how_to_lose_a_guy_in_10_days.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  35%|███▍      | 1619/4637 [00:57<01:58, 25.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Comedy/2004/2480_226704_299940_little_black_book.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  39%|███▉      | 1830/4637 [01:05<01:44, 26.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2003/1723_68784_89159_big_fish.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  46%|████▌     | 2115/4637 [01:13<01:00, 41.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2002/3265_149497_204017_unfaithful.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  47%|████▋     | 2173/4637 [01:14<00:54, 45.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2000/179_88528_119102_batoru_rowaiaru.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  52%|█████▏    | 2431/4637 [01:21<00:37, 59.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2004/146_206647_272090_eternal_sunshine_of_the_spotless_mind.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  63%|██████▎   | 2916/4637 [01:36<00:42, 40.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Family/2001/3935_19508_22105_cats__dogs.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  71%|███████   | 3272/4637 [01:46<00:30, 45.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Horror/1922/1166_134135_184270_nosferatu_eine_symphonie_des_grauens.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  73%|███████▎  | 3407/4637 [01:49<00:27, 44.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Action/2003/602_152466_207871_batoru_rowaiaru_ii_rekuiemu.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  90%|████████▉ | 4165/4637 [02:08<00:08, 57.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Action/2004/59_84873_113518_appurushdo.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files: 100%|██████████| 4637/4637 [02:22<00:00, 32.61it/s]\n",
            "100%|██████████| 1648080/1648080 [00:20<00:00, 79743.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp0PPO8sVYN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = opensubs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fgq-ZAsVYOB",
        "colab_type": "code",
        "colab": {},
        "outputId": "77fb399a-109e-46aa-feeb-f15ee8d5a738"
      },
      "source": [
        "print(len(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "166067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nyV5bxEVYOG",
        "colab_type": "text"
      },
      "source": [
        "# Vocabulary\n",
        "\n",
        "The vocabulary is now created, i.e. an one-hot encoding for each word of the corpus. We have to deal with extra words for padding, start and end of sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVeF_PaVVYOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcsSXlQzVYOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Voc:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        # Extra words for Padding, EOS, SOS\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # count extra words\n",
        "\n",
        "    # Add a sentence to the vocabulary\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    # Add a word to the vocabulary\n",
        "    def addWord(self, word):\n",
        "        # Add non-existing word only\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "payn1xpgVYOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = Voc()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RJHmsytVYOP",
        "colab_type": "text"
      },
      "source": [
        "Words from corpus are added to the vocabury"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sknHklP6VYOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_sequence, output_sequence in pairs:\n",
        "    voc.addSentence(input_sequence)\n",
        "    voc.addSentence(output_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANcbHYspVYOW",
        "colab_type": "code",
        "colab": {},
        "outputId": "ab1b6e47-b330-4dbb-fa0f-9246609c3298"
      },
      "source": [
        "print(\"Counted words:\", voc.num_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counted words: 25523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYU7U0AbVYOZ",
        "colab_type": "text"
      },
      "source": [
        "# Data preparation for batch\n",
        "\n",
        "Batch is a technique to speedup learning of neural networks by packing data and feeding neural networks. To batch sequences, we have to use padding for dealing with different sizes of sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MCVbj7uVYOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zwKJo9OVYOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HguNBR2CVYOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiOOrJg2VYOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYeAKNn6VYOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.ByteTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rj3msIlVYOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZA7nMXlVYOr",
        "colab_type": "text"
      },
      "source": [
        "# RNN with attention\n",
        "\n",
        "References for this part are the following : \n",
        "* seq2seq : https://arxiv.org/abs/1409.3215\n",
        "* encoder bidirectional RNN : https://arxiv.org/pdf/1406.1078v3.pdf\n",
        "* attention : https://arxiv.org/abs/1409.0473\n",
        "* global attention : https://arxiv.org/abs/1508.04025\n",
        "\n",
        "Videos from week 4 and 5 were very usefull to understand attention meaning and how to deploy a solution. Thanks for that :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSirpnwMVYOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = 2\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_dim = embedding.embedding_dim\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # GRU RNN\n",
        "        self.gru = nn.GRU(self.embedding_dim, \n",
        "                          self.hidden_size, \n",
        "                          self.n_layers,\n",
        "                          dropout=0.1, \n",
        "                          bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzc8wUVVYOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the dot method\n",
        "        attn_energies = torch.sum(hidden * encoder_outputs, dim=2)\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXj85fwZVYOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = 2\n",
        "        self.dropout = 0.1\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dim = embedding.embedding_dim\n",
        "        self.embedding_dropout = nn.Dropout(self.dropout)\n",
        "        self.gru = nn.GRU(self.embedding_dim, \n",
        "                          self.hidden_size, \n",
        "                          self.n_layers, \n",
        "                          dropout=0.1)\n",
        "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.attn = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dqX1mnnVYOz",
        "colab_type": "text"
      },
      "source": [
        "# Loss with mask\n",
        "\n",
        "We don't compute loss on padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqkpX4qWVYOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGoqoudfVYO3",
        "colab_type": "text"
      },
      "source": [
        "# Training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwUDtYmkVYO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 500\n",
        "\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "\n",
        "encoder = EncoderRNN(hidden_size, embedding)\n",
        "decoder = DecoderRNN(hidden_size, embedding, voc.num_words)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fn-Uq21VYO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_iteration = 30000\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5l4zn6VVYO-",
        "colab_type": "text"
      },
      "source": [
        "Generate all training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huW9TKVAVYO-",
        "colab_type": "code",
        "colab": {},
        "outputId": "48ec5f75-5022-4685-dfb1-47fc122b8266"
      },
      "source": [
        "# Load batches for each iteration\n",
        "training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in tqdm(range(n_iteration))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:10<00:00, 2946.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o71r_JmFVYPB",
        "colab_type": "code",
        "colab": {},
        "outputId": "65703a96-fb21-4ccc-c409-d9820ce7bf68"
      },
      "source": [
        "print_every = 500\n",
        "start_iteration = 1\n",
        "print_loss = 0\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "for iteration in range(start_iteration, n_iteration + 1):\n",
        "    \n",
        "    training_batch = training_batches[iteration - 1]\n",
        "    \n",
        "    # Extract fields from batch\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    for t in range(max_target_len):\n",
        "        decoder_output, decoder_hidden = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs\n",
        "        )\n",
        "        # next input is current target\n",
        "        decoder_input = target_variable[t].view(1, -1)\n",
        "        # Calculate and accumulate loss\n",
        "        mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "        loss += mask_loss\n",
        "        losses.append(mask_loss.item() * nTotal)\n",
        "        n_totals += nTotal\n",
        "    \n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), 50)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), 50)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    print_loss += sum(losses) / n_totals\n",
        "\n",
        "    # Print progress\n",
        "    if iteration % print_every == 0:\n",
        "        print_loss_avg = print_loss / print_every\n",
        "        print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "        print_loss = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500; Percent complete: 1.7%; Average loss: 5.2885\n",
            "Iteration: 1000; Percent complete: 3.3%; Average loss: 4.6670\n",
            "Iteration: 1500; Percent complete: 5.0%; Average loss: 4.4669\n",
            "Iteration: 2000; Percent complete: 6.7%; Average loss: 4.2793\n",
            "Iteration: 2500; Percent complete: 8.3%; Average loss: 4.1383\n",
            "Iteration: 3000; Percent complete: 10.0%; Average loss: 4.0242\n",
            "Iteration: 3500; Percent complete: 11.7%; Average loss: 3.9140\n",
            "Iteration: 4000; Percent complete: 13.3%; Average loss: 3.8248\n",
            "Iteration: 4500; Percent complete: 15.0%; Average loss: 3.7060\n",
            "Iteration: 5000; Percent complete: 16.7%; Average loss: 3.6189\n",
            "Iteration: 5500; Percent complete: 18.3%; Average loss: 3.5128\n",
            "Iteration: 6000; Percent complete: 20.0%; Average loss: 3.4160\n",
            "Iteration: 6500; Percent complete: 21.7%; Average loss: 3.3521\n",
            "Iteration: 7000; Percent complete: 23.3%; Average loss: 3.2789\n",
            "Iteration: 7500; Percent complete: 25.0%; Average loss: 3.1891\n",
            "Iteration: 8000; Percent complete: 26.7%; Average loss: 3.1254\n",
            "Iteration: 8500; Percent complete: 28.3%; Average loss: 3.0607\n",
            "Iteration: 9000; Percent complete: 30.0%; Average loss: 2.9820\n",
            "Iteration: 9500; Percent complete: 31.7%; Average loss: 2.9163\n",
            "Iteration: 10000; Percent complete: 33.3%; Average loss: 2.8437\n",
            "Iteration: 10500; Percent complete: 35.0%; Average loss: 2.7931\n",
            "Iteration: 11000; Percent complete: 36.7%; Average loss: 2.7322\n",
            "Iteration: 11500; Percent complete: 38.3%; Average loss: 2.6650\n",
            "Iteration: 12000; Percent complete: 40.0%; Average loss: 2.6214\n",
            "Iteration: 12500; Percent complete: 41.7%; Average loss: 2.5848\n",
            "Iteration: 13000; Percent complete: 43.3%; Average loss: 2.5191\n",
            "Iteration: 13500; Percent complete: 45.0%; Average loss: 2.4684\n",
            "Iteration: 14000; Percent complete: 46.7%; Average loss: 2.4246\n",
            "Iteration: 14500; Percent complete: 48.3%; Average loss: 2.3530\n",
            "Iteration: 15000; Percent complete: 50.0%; Average loss: 2.3223\n",
            "Iteration: 15500; Percent complete: 51.7%; Average loss: 2.2743\n",
            "Iteration: 16000; Percent complete: 53.3%; Average loss: 2.2310\n",
            "Iteration: 16500; Percent complete: 55.0%; Average loss: 2.1847\n",
            "Iteration: 17000; Percent complete: 56.7%; Average loss: 2.1407\n",
            "Iteration: 17500; Percent complete: 58.3%; Average loss: 2.1009\n",
            "Iteration: 18000; Percent complete: 60.0%; Average loss: 2.0750\n",
            "Iteration: 18500; Percent complete: 61.7%; Average loss: 2.0140\n",
            "Iteration: 19000; Percent complete: 63.3%; Average loss: 2.0104\n",
            "Iteration: 19500; Percent complete: 65.0%; Average loss: 1.9818\n",
            "Iteration: 20000; Percent complete: 66.7%; Average loss: 1.9255\n",
            "Iteration: 20500; Percent complete: 68.3%; Average loss: 1.8709\n",
            "Iteration: 21000; Percent complete: 70.0%; Average loss: 1.8416\n",
            "Iteration: 21500; Percent complete: 71.7%; Average loss: 1.8301\n",
            "Iteration: 22000; Percent complete: 73.3%; Average loss: 1.7828\n",
            "Iteration: 22500; Percent complete: 75.0%; Average loss: 1.7414\n",
            "Iteration: 23000; Percent complete: 76.7%; Average loss: 1.7218\n",
            "Iteration: 23500; Percent complete: 78.3%; Average loss: 1.7034\n",
            "Iteration: 24000; Percent complete: 80.0%; Average loss: 1.6563\n",
            "Iteration: 24500; Percent complete: 81.7%; Average loss: 1.6303\n",
            "Iteration: 25000; Percent complete: 83.3%; Average loss: 1.6044\n",
            "Iteration: 25500; Percent complete: 85.0%; Average loss: 1.5789\n",
            "Iteration: 26000; Percent complete: 86.7%; Average loss: 1.5553\n",
            "Iteration: 26500; Percent complete: 88.3%; Average loss: 1.5173\n",
            "Iteration: 27000; Percent complete: 90.0%; Average loss: 1.4931\n",
            "Iteration: 27500; Percent complete: 91.7%; Average loss: 1.4732\n",
            "Iteration: 28000; Percent complete: 93.3%; Average loss: 1.4569\n",
            "Iteration: 28500; Percent complete: 95.0%; Average loss: 1.4269\n",
            "Iteration: 29000; Percent complete: 96.7%; Average loss: 1.4034\n",
            "Iteration: 29500; Percent complete: 98.3%; Average loss: 1.3837\n",
            "Iteration: 30000; Percent complete: 100.0%; Average loss: 1.3704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbTUQW3XVYPD",
        "colab_type": "text"
      },
      "source": [
        "# Save network\n",
        "\n",
        "Save results to be used by a chatbot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gIQ9565VYPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save({\n",
        "    'max_len': max_len,\n",
        "    'hidden_size': hidden_size,\n",
        "    'encoder': encoder.state_dict(),\n",
        "    'decoder': decoder.state_dict(),\n",
        "    'voc_dict': voc.__dict__,\n",
        "    'embedding': embedding.state_dict()\n",
        "}, os.path.join('data', 'encoder_decoder.tar'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTBtWk5SVYPG",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsONKwtFVYPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, voc, sentence, max_length=max_len):\n",
        "    # First we check if voc fits to the sentence\n",
        "    for word in sentence.split(' '):\n",
        "        if word not in voc.word2index:\n",
        "            return \"I don't know the meaning of %s\" % word\n",
        "     # Evaluation mode\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Predict\n",
        "    # First, forward input through encoder model\n",
        "    encoder_outputs, encoder_hidden = encoder(input_batch, lengths)\n",
        "    # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "    # Initialize decoder input with SOS_token\n",
        "    decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "    # Initialize tensors to append decoded words to\n",
        "    tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "    # Iteratively decode one word token at a time\n",
        "    for _ in range(max_length):\n",
        "        # Forward pass through decoder\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "        # Obtain most likely word token and its softmax score\n",
        "        _, decoder_input = torch.max(decoder_output, dim=1)\n",
        "        # Record token and score\n",
        "        tokens = torch.cat((tokens, decoder_input), dim=0)\n",
        "        # Prepare current token to be next decoder input (add a dimension)\n",
        "        decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    # Format response sentence \n",
        "    output_words = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD')]\n",
        "    return ' '.join(output_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf8sCcDEVYPL",
        "colab_type": "code",
        "colab": {},
        "outputId": "79c896e3-d70c-4953-ee95-c71e4446d9cf"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('Hello')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEif6vlsVYPN",
        "colab_type": "code",
        "colab": {},
        "outputId": "8231af6b-43f8-4d27-ca78-7f6cc56d490b"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('How are you ?')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i m good and paul\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5llOpjQVYPQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "7e6ba6fc-484f-4b22-d63c-d26023e2c3e5"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('What is your name ?')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "john\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaIDp5XtVYPS",
        "colab_type": "code",
        "colab": {},
        "outputId": "64592b99-9c87-4460-8a1e-11b8d43f7ca9"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('Nice to meet you')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nice to meet you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wxGInsgVYPV",
        "colab_type": "text"
      },
      "source": [
        "The fellowing answer is clearly wrong :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQT9xym0VYPV",
        "colab_type": "code",
        "colab": {},
        "outputId": "89e7d58f-54b9-44ad-f56f-dc4a16a4fb9f"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('Where do you live ?')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "united taxi co\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZgUTPX6VYPZ",
        "colab_type": "text"
      },
      "source": [
        "and so is the fellowing one :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg_HP7AdVYPZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "749be57a-421d-49d5-cdfb-2e72afbd47f7"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('Where do you come from ?')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "when we will come\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya6NmQtxVYPc",
        "colab_type": "code",
        "colab": {},
        "outputId": "f23480d5-8ee9-43e5-ca31-31168f9dc261"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('Do you like apples ?')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I don't know the meaning of apples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l2zJA81VYPh",
        "colab_type": "code",
        "colab": {},
        "outputId": "a4d1193b-ab1c-4ace-abc9-fd70b9c30fe7"
      },
      "source": [
        "print(evaluate(encoder, decoder, voc, datasets.extractText('Do you like apple ?')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yeah\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3N8A2TqVYPj",
        "colab_type": "text"
      },
      "source": [
        "# Using MyChatbot\n",
        "\n",
        "Just few lines to show how to use `mychatbot` running on `telegram` is implemented. Notice that we don't have `GPU` available on `Amazon Web Service` cluster (free offer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6pcNcoeVYPk",
        "colab_type": "code",
        "colab": {},
        "outputId": "4f470ef2-5a3f-4106-a747-c851083a7b48"
      },
      "source": [
        "import mychatbot\n",
        "import importlib\n",
        "importlib.reload(mychatbot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'mychatbot' from '/work/home/tmp/natural-language-processing/honor/mychatbot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aHXeJl6VYPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chatbot = mychatbot.MyChatbot(os.path.join('data', 'encoder_decoder.tar'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3BvbRNvVYPq",
        "colab_type": "code",
        "colab": {},
        "outputId": "358b134b-0ff8-4c02-91dd-febf402aee03"
      },
      "source": [
        "print(chatbot.get_answer('Hello'))\n",
        "print(chatbot.get_answer('How are you ?'))\n",
        "print(chatbot.get_answer('What is your name ?'))\n",
        "print(chatbot.get_answer('Nice to meet you'))\n",
        "print(chatbot.get_answer('Where do you live ?'))\n",
        "print(chatbot.get_answer('Where do you come from ?'))\n",
        "print(chatbot.get_answer('Do you like apples ?'))\n",
        "print(chatbot.get_answer('Do you like apple ?'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "i m good and paul\n",
            "john\n",
            "nice to meet you\n",
            "united taxi co\n",
            "when we will come\n",
            "I don't know the meaning of apples\n",
            "yeah\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooQtLBHuVYPt",
        "colab_type": "text"
      },
      "source": [
        "That's it !! Thanks for reading this notebook so far :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCSncFDwVYPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}